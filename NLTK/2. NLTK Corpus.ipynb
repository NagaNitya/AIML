{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK (Natural Language Toolkit) corpus is a collection of text datasets provided by the NLTK library, which is widely used in natural language processing (NLP). These datasets enable developers to experiment with and train NLP models.\n",
    "\n",
    "It includes:\n",
    "- **Text Data**: Collections of books, news articles, chats, etc. (e.g., \"Gutenberg Corpus,\" \"Chat Corpus\").\n",
    "- **Annotated Corpora**: Tagged and structured text, like part-of-speech-tagged sentences (e.g., \"Treebank\").\n",
    "- **Lexical Resources**: Wordlists and dictionaries (e.g., \"WordNet\").\n",
    "- **Language Models**: Pretrained statistical models for tasks like tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop words are commonly used words in a language that are often excluded from processing because they carry little to no meaningful information. These words are usually so frequent that they can overshadow the more important terms in a text when analyzing or building models.\n",
    "\n",
    "Examples of stop words in English include:\n",
    "\n",
    "+ **Articles**: a, an, the\n",
    "+ **Conjunctions**: and, or, but\n",
    "+ **Prepositions**: in, on, at\n",
    "+ **Pronouns**: he, she, they"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLTK, there is a built-in stopwords corpus that contains predefined lists of stop words for different languages. These lists can be used to filter out common words when processing text data.\n",
    "\n",
    "in order to use this corpus, we first need to download it, then import the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t', \"it's\", 'their', 'some', \"it'll\", 'between', 'd', 'needn', 'o', \"he'd\", \"we've\", 'to', 'yourself', 'there', \"won't\", \"couldn't\", 'so', 'of', 'once', 'themselves', \"we'd\", 'won', 'her', 'not', \"weren't\", 'only', \"we'll\", 'any', 'but', 'how', 'theirs', 'about', 'me', 'what', 'him', 'hadn', 'by', \"needn't\", \"you'd\", 'doing', 'had', 'we', \"doesn't\", 'll', 'in', 'isn', 'she', \"mustn't\", \"she's\", 'aren', 'have', 'was', 'while', 'here', 'that', 'where', 'whom', \"you've\", 'ma', 'they', 'on', 'does', \"he'll\", 'because', \"she'd\", \"they'd\", 'weren', 'as', 'very', 'up', \"hadn't\", \"they're\", \"that'll\", 'has', \"i'm\", 'most', 'than', 'same', 'then', 'and', 'a', 'are', 'i', 'shan', 'out', 'wouldn', 'which', \"didn't\", 'its', 'having', 'those', 'own', 'more', 'when', 'further', 'ourselves', 'our', 'your', \"aren't\", \"you're\", 'until', 'herself', 'during', 'again', \"shouldn't\", 'will', 'through', 'couldn', \"they'll\", 'himself', 'yours', 'who', \"you'll\", 'before', 'over', \"wouldn't\", \"haven't\", 'can', 'did', 'each', 'my', 'or', \"she'll\", 'ours', 'his', \"isn't\", \"hasn't\", \"i'd\", \"don't\", \"i'll\", 'after', \"it'd\", 'now', 'it', \"i've\", 'at', 'm', \"mightn't\", 'myself', 'into', \"shan't\", 'doesn', 'mustn', 'ain', 'hasn', \"should've\", 'should', 'itself', 'being', 'all', 'under', 's', 'haven', 'why', 'am', 'them', 'shouldn', 'be', 'from', 'don', 'these', 'he', \"wasn't\", 'been', \"he's\", 've', 'didn', 'too', 'such', 'just', 'mightn', 'few', 'for', 'hers', 'if', 'off', 'other', 'this', 'both', \"we're\", 'do', 'you', 'with', 'no', 'an', 'below', \"they've\", 'is', 'against', 'down', 'the', 'yourselves', 're', 'y', 'were', 'wasn', 'above', 'nor'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing stop words, we reduce the noise in the text data and focus on the words that carry more significant meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence=\"This is a sample sentence, showing off the stop words filtration.\"\n",
    "words=word_tokenize(sentence)\n",
    "filtered_sentence = [w for w in words if not w in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in a similar manner to stopwords, other corpora may be downloaded and imported from the `nltk.corpus` package, for use in various applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*WordNet* is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. One can use it to find the synonyms, antonyms and definitions of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nagan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('computer.n.01'), Synset('calculator.n.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns=wordnet.synsets(\"computer\")\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the word itself, as well as the definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer\n",
      "a machine for performing calculations automatically\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].lemmas()[0].name())\n",
    "print(syns[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "syns1=wordnet.synsets(\"program\")\n",
    "print(syns1[0].examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the synonyms and antonyms of a word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dear', 'honest', 'unspoilt', 'commodity', 'thoroughly', 'adept', 'good', 'safe', 'expert', 'salutary', 'well', 'practiced', 'serious', 'honorable', 'full', 'just', 'goodness', 'respectable', 'ripe', 'in_force', 'undecomposed', 'trade_good', 'secure', 'upright', 'unspoiled', 'in_effect', 'skillful', 'effective', 'right', 'dependable', 'beneficial', 'estimable', 'sound', 'near', 'proficient', 'skilful', 'soundly'}\n",
      "{'ill', 'bad', 'badness', 'evil', 'evilness'}\n"
     ]
    }
   ],
   "source": [
    "synonyms=[]\n",
    "antonyms=[]\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "print(set(synonyms))\n",
    "print(set(antonyms))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the semantic similarity between 2 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "w1=wordnet.synset(\"ship.n.01\")\n",
    "w2=wordnet.synset(\"boat.n.01\")\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [sentdex-corpora](https://youtu.be/TKAXDqoG2dc)\n",
    "+ [sentdex-Wordnet](https://youtu.be/T68P5-8tM-Y)\n",
    "+ [Princeton.edu](https://wordnet.princeton.edu/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
