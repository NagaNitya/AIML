{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual layers are an important part of deep networks which help with the vanishing gradient problem. These are layers which can be skipped, ie. the output of one layer goes to the layer after the next. Hence the flow of information during backpropogation becomes more efficient, as well as the extraction of features by the network. Eg. ResNet.\n",
    "\n",
    "Essentially a short cut route is given for the gradient to flow, thereby reducing the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def res-block(l_in, filters):\n",
    "    prev_layer=Conv2D(filters[0], (3, 3), padding='same', activstion='relu)(l_in)\n",
    "    for f in range(1, len(filters)):\n",
    "        conv=Conv2D(filters[f], (3, 3), padding='same', activation='relu')(p_l)\n",
    "        p_l=Add()([prev_layer, conv])\n",
    "    return p_l"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
